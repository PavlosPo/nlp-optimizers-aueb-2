{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ppoulos/miniconda3/envs/pavlosEnv2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"cnn_dailymail\"\n",
    "seed_num = 1\n",
    "model_name = \"google-t5/t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dataset = load_dataset(dataset, '3.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 287113\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 13368\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 11490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dataset\n",
    "# make the dataset into a pandas dataframe\n",
    "# df = pd.DataFrame(loaded_dataset['train'])\n",
    "# # add the test dataset to the dataframe\n",
    "# df = pd.concat([df, pd.DataFrame(loaded_dataset['test'])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the summary column\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 287113/287113 [03:43<00:00, 1285.23 examples/s]\n",
      "Map: 100%|██████████| 13368/13368 [00:10<00:00, 1222.92 examples/s]\n",
      "Map: 100%|██████████| 11490/11490 [00:09<00:00, 1227.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "prefix = \"summarize: \"  # Required so the T5 model knows that we are going to summarize\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(text_target=examples[\"highlights\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)\n",
    "tokenized_dataset = loaded_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>id</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287108</th>\n",
       "      <td>The nine-year-old daughter of a black, unarmed...</td>\n",
       "      <td>Rumain Brisbon, 34, was killed after Phoenix p...</td>\n",
       "      <td>279a12d3ee37b8109cc192a9e88115a5a631fb06</td>\n",
       "      <td>[21603, 10, 37, 4169, 18, 1201, 18, 1490, 3062...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2770, 7484, 7834, 7, 5407, 6, 6154, 6, 47, 47...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287109</th>\n",
       "      <td>Legalising assisted suicide is a slippery slop...</td>\n",
       "      <td>Theo Boer, a European assisted suicide watchdo...</td>\n",
       "      <td>b5bc9d404a9a5d890c9fc26550b67e6d8d83241f</td>\n",
       "      <td>[21603, 10, 11281, 4890, 11752, 12259, 19, 3, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[37, 32, 1491, 49, 6, 3, 9, 1611, 11752, 12259...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287110</th>\n",
       "      <td>A group calling itself 'The Women of the 99 Pe...</td>\n",
       "      <td>Ohio congressman criticised for 'condoning the...</td>\n",
       "      <td>500862586f925e406f8b662934e1a71bbee32463</td>\n",
       "      <td>[21603, 10, 71, 563, 3874, 1402, 3, 31, 634, 4...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[6167, 27197, 348, 6800, 3375, 21, 3, 31, 1018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287111</th>\n",
       "      <td>Most men enjoy a good pint of lager or real al...</td>\n",
       "      <td>The Black Country Ale Tairsters have been to 1...</td>\n",
       "      <td>32a1f9e5c37a938c0c0bca1a1559247b9c4334b2</td>\n",
       "      <td>[21603, 10, 1377, 1076, 777, 3, 9, 207, 4522, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[37, 1589, 6993, 15345, 332, 2256, 1370, 7, 43...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287112</th>\n",
       "      <td>A Facebook page seeking to preserve the 'Black...</td>\n",
       "      <td>Facebook page supporting tradition gains one m...</td>\n",
       "      <td>8ec9ff4d633dd4cc26d53f503c33f7464b43c36e</td>\n",
       "      <td>[21603, 10, 71, 1376, 543, 3945, 12, 8996, 8, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1376, 543, 3956, 4387, 11391, 80, 770, 3, 31,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  article  \\\n",
       "287108  The nine-year-old daughter of a black, unarmed...   \n",
       "287109  Legalising assisted suicide is a slippery slop...   \n",
       "287110  A group calling itself 'The Women of the 99 Pe...   \n",
       "287111  Most men enjoy a good pint of lager or real al...   \n",
       "287112  A Facebook page seeking to preserve the 'Black...   \n",
       "\n",
       "                                               highlights  \\\n",
       "287108  Rumain Brisbon, 34, was killed after Phoenix p...   \n",
       "287109  Theo Boer, a European assisted suicide watchdo...   \n",
       "287110  Ohio congressman criticised for 'condoning the...   \n",
       "287111  The Black Country Ale Tairsters have been to 1...   \n",
       "287112  Facebook page supporting tradition gains one m...   \n",
       "\n",
       "                                              id  \\\n",
       "287108  279a12d3ee37b8109cc192a9e88115a5a631fb06   \n",
       "287109  b5bc9d404a9a5d890c9fc26550b67e6d8d83241f   \n",
       "287110  500862586f925e406f8b662934e1a71bbee32463   \n",
       "287111  32a1f9e5c37a938c0c0bca1a1559247b9c4334b2   \n",
       "287112  8ec9ff4d633dd4cc26d53f503c33f7464b43c36e   \n",
       "\n",
       "                                                input_ids  \\\n",
       "287108  [21603, 10, 37, 4169, 18, 1201, 18, 1490, 3062...   \n",
       "287109  [21603, 10, 11281, 4890, 11752, 12259, 19, 3, ...   \n",
       "287110  [21603, 10, 71, 563, 3874, 1402, 3, 31, 634, 4...   \n",
       "287111  [21603, 10, 1377, 1076, 777, 3, 9, 207, 4522, ...   \n",
       "287112  [21603, 10, 71, 1376, 543, 3945, 12, 8996, 8, ...   \n",
       "\n",
       "                                           attention_mask  \\\n",
       "287108  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "287109  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "287110  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "287111  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "287112  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                   labels  \n",
       "287108  [2770, 7484, 7834, 7, 5407, 6, 6154, 6, 47, 47...  \n",
       "287109  [37, 32, 1491, 49, 6, 3, 9, 1611, 11752, 12259...  \n",
       "287110  [6167, 27197, 348, 6800, 3375, 21, 3, 31, 1018...  \n",
       "287111  [37, 1589, 6993, 15345, 332, 2256, 1370, 7, 43...  \n",
       "287112  [1376, 543, 3956, 4387, 11391, 80, 770, 3, 31,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the dataset into a Dataframe\n",
    "df = pd.DataFrame(tokenized_dataset['train'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access '\n",
      " 'to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, '\n",
      " \"but he insists the money won't cast a spell on him. Daniel Radcliffe as \"\n",
      " 'Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the '\n",
      " 'disappointment of gossip columnists around the world, the young actor says '\n",
      " 'he has no plans to fritter his cash away on fast cars, drink and celebrity '\n",
      " 'parties. \"I don\\'t plan to be one of those people who, as soon as they turn '\n",
      " '18, suddenly buy themselves a massive sports car collection or something '\n",
      " 'similar,\" he told an Australian interviewer earlier this month. \"I don\\'t '\n",
      " 'think I\\'ll be particularly extravagant. \"The things I like buying are '\n",
      " 'things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, '\n",
      " 'Radcliffe will be able to gamble in a casino, buy a drink in a pub or see '\n",
      " 'the horror film \"Hostel: Part II,\" currently six places below his number one '\n",
      " \"movie on the UK box office chart. Details of how he'll mark his landmark \"\n",
      " 'birthday are under wraps. His agent and publicist had no comment on his '\n",
      " 'plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. '\n",
      " '\"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from '\n",
      " 'the first five Potter films have been held in a trust fund which he has not '\n",
      " 'been able to touch. Despite his growing fame and riches, the actor says he '\n",
      " 'is keeping his feet firmly on the ground. \"People are always looking to say '\n",
      " '\\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try '\n",
      " 'very hard not to go that way because it would be too easy for them.\" His '\n",
      " 'latest outing as the boy wizard in \"Harry Potter and the Order of the '\n",
      " 'Phoenix\" is breaking records on both sides of the Atlantic and he will '\n",
      " 'reprise the role in the last two films.  Watch I-Reporter give her review of '\n",
      " \"Potter's latest » . There is life beyond Potter, however. The Londoner has \"\n",
      " 'filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his '\n",
      " 'son, due for release later this year. He will also appear in \"December '\n",
      " 'Boys,\" an Australian film about four boys who escape an orphanage. Earlier '\n",
      " 'this year, he made his stage debut playing a tortured teenager in Peter '\n",
      " 'Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny '\n",
      " 'now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of '\n",
      " 'fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. '\n",
      " 'All rights reserved.This material may not be published, broadcast, '\n",
      " 'rewritten, or redistributed.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(df['article'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    287113.000000\n",
       "mean        985.055038\n",
       "std         480.603123\n",
       "min          20.000000\n",
       "25%         631.000000\n",
       "50%         898.000000\n",
       "75%        1244.000000\n",
       "90%        1659.000000\n",
       "95%        1947.000000\n",
       "99%        2405.000000\n",
       "max        5269.000000\n",
       "Name: input_ids, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give me the percentiles of length of input_ids using pandas and plot them\n",
    "df['input_ids'].apply(len).describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    287113.000000\n",
       "mean         74.682811\n",
       "std          30.752373\n",
       "min           7.000000\n",
       "25%          55.000000\n",
       "50%          70.000000\n",
       "75%          87.000000\n",
       "90%         110.000000\n",
       "95%         129.000000\n",
       "99%         171.000000\n",
       "max        3151.000000\n",
       "Name: labels, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The same for the labels\n",
    "df['labels'].apply(len).describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ppoulos/miniconda3/envs/pavlosEnv2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ic| len(train): 199\n",
      "ic| len(val): 99\n",
      "ic| len(test): 100\n",
      "/home/ppoulos/miniconda3/envs/pavlosEnv2/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The argument `model_name_or_path` was not specified while it is required when the default `transformers` model is used. It will use the default recommended model - 'roberta-large'.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/ppoulos/miniconda3/envs/pavlosEnv2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 199/199 [00:00<00:00, 1387.86 examples/s]\n",
      "Map: 100%|██████████| 99/99 [00:00<00:00, 1392.48 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1593.40 examples/s]\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizer: Adam with name: adam was created.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpavlospoulos\u001b[0m (\u001b[33mpavlos_poulos\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ppoulos/nlp-optimizers-aueb-2/cnn_dailymail/wandb/run-20240621_170725-vk8j0aks</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pavlos_poulos/huggingface/runs/vk8j0aks' target=\"_blank\">adam-cnn_dailymail-t5</a></strong> to <a href='https://wandb.ai/pavlos_poulos/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pavlos_poulos/huggingface' target=\"_blank\">https://wandb.ai/pavlos_poulos/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pavlos_poulos/huggingface/runs/vk8j0aks' target=\"_blank\">https://wandb.ai/pavlos_poulos/huggingface/runs/vk8j0aks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1 Fmeasure</th>\n",
       "      <th>Rouge1 Precision</th>\n",
       "      <th>Rouge1 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rougel Fmeasure</th>\n",
       "      <th>Rougel Precision</th>\n",
       "      <th>Rougel Recall</th>\n",
       "      <th>Rougelsum Fmeasure</th>\n",
       "      <th>Rougelsum Precision</th>\n",
       "      <th>Rougelsum Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.615400</td>\n",
       "      <td>2.540219</td>\n",
       "      <td>0.242623</td>\n",
       "      <td>0.413168</td>\n",
       "      <td>0.176543</td>\n",
       "      <td>0.092289</td>\n",
       "      <td>0.157918</td>\n",
       "      <td>0.066812</td>\n",
       "      <td>0.198366</td>\n",
       "      <td>0.337661</td>\n",
       "      <td>0.144396</td>\n",
       "      <td>0.220622</td>\n",
       "      <td>0.375273</td>\n",
       "      <td>0.160574</td>\n",
       "      <td>0.962894</td>\n",
       "      <td>0.953787</td>\n",
       "      <td>0.958302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.591800</td>\n",
       "      <td>2.500165</td>\n",
       "      <td>0.245461</td>\n",
       "      <td>0.419882</td>\n",
       "      <td>0.178369</td>\n",
       "      <td>0.093790</td>\n",
       "      <td>0.161489</td>\n",
       "      <td>0.067783</td>\n",
       "      <td>0.202886</td>\n",
       "      <td>0.346387</td>\n",
       "      <td>0.147604</td>\n",
       "      <td>0.223479</td>\n",
       "      <td>0.381626</td>\n",
       "      <td>0.162504</td>\n",
       "      <td>0.962990</td>\n",
       "      <td>0.953780</td>\n",
       "      <td>0.958346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ppoulos/miniconda3/envs/pavlosEnv2/lib/python3.11/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from torchmetrics.text.bert import BERTScore\n",
    "from evaluate import load\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "import numpy as np\n",
    "import wandb\n",
    "import os\n",
    "from icecream import ic\n",
    "wandb.require(\"core\")\n",
    "\n",
    "# Parameters for the rest of the script\n",
    "optimizer_name = \"adam\"\n",
    "model_name = \"google-t5/t5-small\"\n",
    "dataset =   \"cnn_dailymail\"\n",
    "seed_num = 1\n",
    "max_length = 512\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wandb_run_name = f\"{optimizer_name}-{dataset}-{model_name.split('-')[1].split('/')[0]}\"\n",
    "output_dir = f\"{optimizer_name}/{dataset}/{model_name.split('-')[1].split('/')[0]}\"\n",
    "\n",
    "\n",
    "# Main\n",
    "# Load the T5 model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Dataset\n",
    "loaded_dataset = load_dataset(dataset, '3.0.0')\n",
    "# loaded_dataset = loaded_dataset.train_test_split(test_size=0.2, seed=seed_num, shuffle=True)\n",
    "train = loaded_dataset['train'].select(range(1, 200)) # Train Dataset 80%\n",
    "temp = loaded_dataset['test'].select(range(1, 200)).train_test_split(test_size=0.5)  # Ignore\n",
    "test = temp['test'] # Test Dataset\n",
    "val = temp['train'] # Val Dataset\n",
    "\n",
    "ic(len(train))\n",
    "ic(len(val))\n",
    "ic(len(test))\n",
    "\n",
    "# Load evaluation\n",
    "rouge = ROUGEScore(use_stemmer=True)\n",
    "bert_score = BERTScore(device=device)\n",
    "# bert_score = load(\"bertscore\")\n",
    "\n",
    "prefix = \"summarize: \"  # Required so the T5 model knows that we are going to summarize\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True)\n",
    "    labels = tokenizer(text_target=examples[\"highlights\"], max_length=max_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)\n",
    "tokenized_dataset_train = train.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_val = val.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_test = test.map(preprocess_function, batched=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result_rouge = rouge(preds=decoded_preds, target=decoded_labels)\n",
    "    result_brt = bert_score(preds=decoded_preds, target=decoded_labels)\n",
    "    result_brt_average_values = {key: tensors.mean().item() for key, tensors in result_brt.items()}\n",
    "    results = {**result_rouge, **result_brt_average_values}\n",
    "    return results\n",
    "\n",
    "def get_optimizer(optimizer_name, model, learning_rate):\n",
    "    if optimizer_name == \"adamw\":\n",
    "        return torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        return torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == \"adam\":\n",
    "        return torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_steps = 20,\n",
    "    eval_steps =20,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    # save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    seed=seed_num,\n",
    "    data_seed=seed_num,\n",
    "    fp16=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=wandb_run_name,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'loss',\n",
    ")\n",
    "\n",
    "class CustomTrainer(Seq2SeqTrainer):\n",
    "    def create_optimizer(self):\n",
    "        self.optimizer = get_optimizer(optimizer_name, self.model, self.args.learning_rate)\n",
    "        print(f\"\\nOptimizer: {self.optimizer.__class__.__name__} with name: {optimizer_name} was created.\\n\")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_train,\n",
    "    eval_dataset=tokenized_dataset_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ppoulos/miniconda3/envs/pavlosEnv2/lib/python3.11/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "results = trainer.predict(tokenized_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 2.4009511470794678,\n",
       " 'test_rouge1_fmeasure': 0.24885646998882294,\n",
       " 'test_rouge1_precision': 0.4333604574203491,\n",
       " 'test_rouge1_recall': 0.17930950224399567,\n",
       " 'test_rouge2_fmeasure': 0.08461087197065353,\n",
       " 'test_rouge2_precision': 0.1504550576210022,\n",
       " 'test_rouge2_recall': 0.060310978442430496,\n",
       " 'test_rougeL_fmeasure': 0.19637919962406158,\n",
       " 'test_rougeL_precision': 0.3421281576156616,\n",
       " 'test_rougeL_recall': 0.14131340384483337,\n",
       " 'test_rougeLsum_fmeasure': 0.22376030683517456,\n",
       " 'test_rougeLsum_precision': 0.39116519689559937,\n",
       " 'test_rougeLsum_recall': 0.16092655062675476,\n",
       " 'test_precision': 0.9636220335960388,\n",
       " 'test_recall': 0.9535024166107178,\n",
       " 'test_f1': 0.9585170149803162,\n",
       " 'test_runtime': 10.9564,\n",
       " 'test_samples_per_second': 9.127,\n",
       " 'test_steps_per_second': 2.282}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ppoulos/miniconda3/envs/pavlosEnv2/lib/python3.11/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** test metrics *****\n",
      "  test_f1                  =     0.9585\n",
      "  test_loss                =      2.401\n",
      "  test_precision           =     0.9636\n",
      "  test_recall              =     0.9535\n",
      "  test_rouge1_fmeasure     =     0.2489\n",
      "  test_rouge1_precision    =     0.4334\n",
      "  test_rouge1_recall       =     0.1793\n",
      "  test_rouge2_fmeasure     =     0.0846\n",
      "  test_rouge2_precision    =     0.1505\n",
      "  test_rouge2_recall       =     0.0603\n",
      "  test_rougeL_fmeasure     =     0.1964\n",
      "  test_rougeL_precision    =     0.3421\n",
      "  test_rougeL_recall       =     0.1413\n",
      "  test_rougeLsum_fmeasure  =     0.2238\n",
      "  test_rougeLsum_precision =     0.3912\n",
      "  test_rougeLsum_recall    =     0.1609\n",
      "  test_runtime             = 0:00:10.40\n",
      "  test_samples_per_second  =      9.612\n",
      "  test_steps_per_second    =      2.403\n"
     ]
    }
   ],
   "source": [
    "# Test the model and save results in file and wandb\n",
    "def test_model():\n",
    "    results = trainer.predict(tokenized_dataset_test)\n",
    "    metrics = results[2]\n",
    "    \n",
    "    # Optionally, save metrics to a file as well\n",
    "    with open(f\"{output_dir}/results.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join([f\"{metric} : {value}\" for metric, value in metrics.items()]))\n",
    "\n",
    "    trainer.log_metrics(\"test\", metrics)\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pavlosEnv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
